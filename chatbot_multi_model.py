import os
import streamlit as st
from dotenv import load_dotenv

# LangChain Imports
from langchain_groq import ChatGroq
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableWithMessageHistory
from langchain.memory import ChatMessageHistory

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# --- 1. THI·∫æT L·∫¨P C·∫§U H√åNH V√Ä H·∫∞NG S·ªê ---

SESSION_ID = "multi_llm_session"
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Kh·ªüi t·∫°o session state cho l·ªãch s·ª≠ tr√≤ chuy·ªán (d·∫°ng dictionary)
if "chat_history_store" not in st.session_state:
    st.session_state.chat_history_store = {}

# Kh·ªüi t·∫°o session state cho hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán (d·∫°ng list)
if "messages" not in st.session_state:
    st.session_state.messages = []


# --- 2. H√ÄM QU·∫¢N L√ù B·ªò NH·ªö ---

def get_session_history(session_id: str) -> ChatMessageHistory:
    """T·∫°o ho·∫∑c tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng ChatMessageHistory t·ª´ Streamlit session state."""
    if session_id not in st.session_state.chat_history_store:
        st.session_state.chat_history_store[session_id] = ChatMessageHistory()
    return st.session_state.chat_history_store[session_id]


# --- 3. H√ÄM KH·ªûI T·∫†O MODEL ƒê·ªòNG ---

def initialize_llm(llm_choice, temperature, max_tokens):
    """Kh·ªüi t·∫°o v√† tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng LLM d·ª±a tr√™n l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi d√πng."""

    # Chu·∫©n h√≥a c√°c tham s·ªë ƒë·ªÉ truy·ªÅn v√†o model
    config_params = {
        "temperature": temperature,
        "max_tokens": max_tokens,
    }

    if llm_choice == "Groq (Llama 3.1 8B)":
        if not GROQ_API_KEY:
            st.warning("Thi·∫øu GROQ_API_KEY. Vui l√≤ng nh·∫≠p kh√≥a API.")
            return None
        return ChatGroq(model="llama-3.1-8b-instant", groq_api_key=GROQ_API_KEY, **config_params)

    elif llm_choice == "Gemini (2.5 Flash)":
        if not GEMINI_API_KEY:
            st.warning("Thi·∫øu GEMINI_API_KEY. Vui l√≤ng nh·∫≠p kh√≥a API.")
            return None
        return ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GEMINI_API_KEY, **config_params)

    elif llm_choice == "Ollama (gemma3)":
        # Ollama ch·∫°y c·ª•c b·ªô, kh√¥ng c·∫ßn API key
        return ChatOllama(model="gemma3:1b", **config_params)

    return None


# --- 4. C·∫§U H√åNH GIAO DI·ªÜN STREAMLIT ---

st.set_page_config(page_title="Khung Chat ƒêa LLM", layout="wide")
st.title("ü§ñ Striker1.0")
st.caption("Ch·ªçn model, ƒëi·ªÅu ch·ªânh tham s·ªë, v√† b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán.")

# Sidebar ƒë·ªÉ l·ª±a ch·ªçn Model v√† Tham s·ªë
with st.sidebar:
    st.header("C·∫•u h√¨nh Model")

    llm_choice = st.selectbox(
        "Ch·ªçn Model LLM:",
        ("Groq (Llama 3.1 8B)", "Gemini (2.5 Flash)", "Ollama (gemma3)")
    )

    st.subheader("ƒêi·ªÅu ch·ªânh Tham s·ªë")

    # C√°c tham s·ªë cho t·∫•t c·∫£ c√°c model
    temperature = st.slider("Temperature", min_value=0.0, max_value=1.0, value=0.7, step=0.05)
    max_tokens = st.slider("Max Tokens", min_value=50, max_value=2048, value=512, step=50)

    st.markdown("---")
    st.info("L∆∞u √Ω: ƒêi·ªÅu ch·ªânh Temperature, Max tokens ƒë·ªÉ tƒÉng s√°ng t·∫°o, ƒë·ªô d√†i c√¢u tr·∫£ l·ªùi .")

# --- 5. H√ÄM CH·∫†Y CHAIN V√Ä GIAO DI·ªÜN CH√çNH ---

# 1. Kh·ªüi t·∫°o LLM v√† Chain m·ªói khi tham s·ªë thay ƒë·ªïi
llm = initialize_llm(llm_choice, temperature, max_tokens)

if llm:
    # ƒê·ªãnh nghƒ©a Prompt Template (ch·ªâ c·∫ßn l√†m m·ªôt l·∫ßn)
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         f"You are a helpful assistant running on {llm_choice}. Keep your answers concise and use the chat history to maintain context."),
        MessagesPlaceholder(variable_name="history"),
        ("user", "Question: {question}")
    ])

    # T·∫°o Core Chain v√† History Chain
    core_chain = prompt | llm
    history_chain = RunnableWithMessageHistory(
        core_chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="history",
    )

    # 2. Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë√£ l∆∞u
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 3. X·ª≠ l√Ω Input c·ªßa ng∆∞·ªùi d√πng
    if prompt_input := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):

        # Th√™m tin nh·∫Øn User v√†o l·ªãch s·ª≠ hi·ªÉn th·ªã
        st.session_state.messages.append({"role": "user", "content": prompt_input})

        with st.chat_message("user"):
            st.markdown(prompt_input)

        # G·ªçi LLM v√† hi·ªÉn th·ªã ph·∫£n h·ªìi c·ªßa AI
        with st.chat_message("assistant"):
            with st.spinner(f"ƒêang h·ªèi {llm_choice}..."):
                try:
                    # G·ªçi h√†m generate_response tr·ª±c ti·∫øp b·∫±ng chain ƒë√£ t·∫°o
                    config = {"configurable": {"session_id": SESSION_ID}}
                    response = history_chain.invoke({"question": prompt_input}, config=config).content

                    st.markdown(response)

                    # Th√™m tin nh·∫Øn c·ªßa AI v√†o l·ªãch s·ª≠ hi·ªÉn th·ªã
                    st.session_state.messages.append({"role": "assistant", "content": response})

                except Exception as e:
                    error_msg = f"ƒê√£ x·∫£y ra l·ªói khi g·ªçi {llm_choice}. Chi ti·∫øt: {e}"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
else:
    st.warning("Vui l√≤ng thi·∫øt l·∫≠p kh√≥a API ho·∫∑c ƒë·∫£m b·∫£o Ollama ƒëang ch·∫°y ƒë·ªÉ s·ª≠ d·ª•ng chatbot.")