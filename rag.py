import os
import streamlit as st
from dotenv import load_dotenv
import tempfile

# LangChain RAG/Embedding Imports
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain, create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain

# LangChain Chat Model/Core Imports
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_groq import ChatGroq
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableWithMessageHistory
from langchain.memory import ChatMessageHistory

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# --- 1. THI·∫æT L·∫¨P C·∫§U H√åNH V√Ä H·∫∞NG S·ªê ---

SESSION_ID = "multi_llm_session"
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# T√™n model Ollama ƒë∆∞·ª£c ch·ªçn
OLLAMA_CHAT_MODEL = "gemma:2b"
OLLAMA_EMBEDDING_MODEL = "nomic-embed-text"

# Kh·ªüi t·∫°o session state
if "chat_history_store" not in st.session_state:
    st.session_state.chat_history_store = {}
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None


# --- 2. H√ÄM QU·∫¢N L√ù B·ªò NH·ªö V√Ä RAG ---

def get_session_history(session_id: str) -> ChatMessageHistory:
    """T·∫°o ho·∫∑c tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng ChatMessageHistory t·ª´ Streamlit session state."""
    if session_id not in st.session_state.chat_history_store:
        st.session_state.chat_history_store[session_id] = ChatMessageHistory()
    return st.session_state.chat_history_store[session_id]


def process_uploaded_file(uploaded_file):
    """T·∫£i, t√°ch v√† t·∫°o FAISS Vector Store t·ª´ file PDF b·∫±ng Ollama Embeddings."""

    # Ghi file t·∫°m th·ªùi
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(uploaded_file.read())
        temp_file_path = tmp_file.name

    try:
        # 1. Load v√† Split t√†i li·ªáu
        loader = PyPDFLoader(temp_file_path)
        documents = loader.load()
        # S·ª≠ d·ª•ng RecursiveCharacterTextSplitter
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)
        chunks = text_splitter.split_documents(documents)

        # 2. T·∫°o Embeddings b·∫±ng OLLAMA
        embeddings = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL)
        vector_store = FAISS.from_documents(chunks, embeddings)
        st.success(f"T·∫£i l√™n v√† x·ª≠ l√Ω file PDF th√†nh c√¥ng! ƒê√£ d√πng {OLLAMA_EMBEDDING_MODEL} cho Embeddings.")
        return vector_store

    except Exception as e:
        st.error(
            f"L·ªói khi x·ª≠ l√Ω file PDF b·∫±ng Ollama: {e}. ƒê·∫£m b·∫£o Ollama ƒëang ch·∫°y v√† model '{OLLAMA_EMBEDDING_MODEL}' ƒë√£ ƒë∆∞·ª£c pull.")
        return None
    finally:
        os.remove(temp_file_path)


# --- 3. H√ÄM KH·ªûI T·∫†O MODEL ƒê·ªòNG ---

def initialize_llm(llm_choice, temperature, max_tokens):
    """Kh·ªüi t·∫°o v√† tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng LLM d·ª±a tr√™n l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi d√πng."""

    config_params = {
        "temperature": temperature,
        "max_tokens": max_tokens,
    }

    if llm_choice == "Groq (Llama 3.1 8B)":
        if not GROQ_API_KEY: return None
        return ChatGroq(model="llama-3.1-8b-instant", groq_api_key=GROQ_API_KEY, **config_params)

    elif llm_choice == "Gemini (2.5 Flash)":
        if not GEMINI_API_KEY: return None
        return ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GEMINI_API_KEY, **config_params)

    elif llm_choice == f"Ollama ({OLLAMA_CHAT_MODEL})":
        # S·ª≠ d·ª•ng model gemma:2b cho Chat
        return ChatOllama(model=OLLAMA_CHAT_MODEL, **config_params)

    return None


# --- 4. C·∫§U H√åNH GIAO DI·ªÜN STREAMLIT ---

st.set_page_config(page_title="Khung Chat RAG ƒêa LLM", layout="wide")
st.title("üìö Chatbot RAG ƒêa M√¥ H√¨nh (PDF Q&A)")
st.caption("Ch·ªçn model, t·∫£i t√†i li·ªáu PDF (s·ª≠ d·ª•ng Ollama Embeddings), v√† b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán.")

# Sidebar ƒë·ªÉ l·ª±a ch·ªçn Model, Tham s·ªë v√† T·∫£i file
with st.sidebar:
    st.header("C·∫•u h√¨nh Model")

    llm_choice = st.selectbox(
        "Ch·ªçn Model LLM:",
        ("Groq (Llama 3.1 8B)", "Gemini (2.5 Flash)", f"Ollama ({OLLAMA_CHAT_MODEL})")
    )

    st.subheader("ƒêi·ªÅu ch·ªânh Tham s·ªë")
    temperature = st.slider("Temperature", min_value=0.0, max_value=1.0, value=0.5, step=0.05)
    max_tokens = st.slider("Max Tokens", min_value=50, max_value=4096, value=1024, step=50)

    st.markdown("---")
    st.header("T√†i li·ªáu RAG (Ollama Embeddings)")
    st.caption(f"Embeddings: **{OLLAMA_EMBEDDING_MODEL}**")

    uploaded_file = st.file_uploader("T·∫£i l√™n PDF c·ªßa b·∫°n:", type="pdf")

    if uploaded_file:
        if st.button("X·ª≠ l√Ω File PDF"):
            st.session_state.vector_store = process_uploaded_file(uploaded_file)

    # Hi·ªÉn th·ªã tr·∫°ng th√°i RAG
    if st.session_state.vector_store:
        st.success("‚úÖ RAG Active: S·∫µn s√†ng h·ªèi v·ªÅ t√†i li·ªáu!")
        st.caption(f"Vector Store ƒë√£ l∆∞u tr·ªØ {len(st.session_state.vector_store.docstore._dict)} chunks.")
    else:
        st.info("RAG Inactive: Chatbot ho·∫°t ƒë·ªông ·ªü ch·∫ø ƒë·ªô th∆∞·ªùng.")

# --- 5. T·∫†O CHAIN T√ôY THU·ªòC V√ÄO TR·∫†NG TH√ÅI RAG ---

llm = initialize_llm(llm_choice, temperature, max_tokens)
history_chain = None

if llm:
    # 1. T·∫°o Prompt c∆° b·∫£n
    if st.session_state.vector_store:
        # Prompt cho RAG
        # CH·ªà S·ª¨ D·ª§NG {context} V√Ä {input} ƒë·ªÉ gi·∫£i quy·∫øt l·ªói truy·ªÅn bi·∫øn
        prompt_template = ChatPromptTemplate.from_messages([
            ("system",
             "You are a helpful assistant. Answer the user's question only based on the retrieved context below. The question has been rephrased based on chat history. If you can't find the answer, state that clearly.\n\nContext: {context}"),
            ("user", "Question: {input}")
        ])

        # T·∫°o History-Aware Retriever (S·ª≠ d·ª•ng {history} v√† {input} ·ªü ƒë√¢y)
        history_aware_prompt = ChatPromptTemplate.from_messages([
            MessagesPlaceholder(variable_name="history"),
            ("user", "Question: {input}"),
            ("user",
             "Given the above conversation, generate a search query to look up in the documents to answer the latest user question.")
        ])

        retriever = st.session_state.vector_store.as_retriever()
        history_aware_retriever = create_history_aware_retriever(llm, retriever, history_aware_prompt)
        document_chain = create_stuff_documents_chain(llm, prompt_template)
        history_chain = create_retrieval_chain(history_aware_retriever, document_chain)

    else:
        # Prompt cho ch·∫ø ƒë·ªô Chat th∆∞·ªùng (Gi·ªØ nguy√™n)
        prompt_template = ChatPromptTemplate.from_messages([
            ("system",
             f"You are a helpful assistant running on {llm_choice}. Keep your answers concise and use the chat history to maintain context."),
            MessagesPlaceholder(variable_name="history"),
            ("user", "Question: {input}")
        ])

        core_chain = prompt_template | llm
        history_chain = RunnableWithMessageHistory(
            core_chain,
            get_session_history,
            input_messages_key="input",
            history_messages_key="history",
        )

# --- 6. GIAO DI·ªÜN CH√çNH (INVOKE) ---

# if llm:
#     for message in st.session_state.messages:
#         with st.chat_message(message["role"]):
#             st.markdown(message["content"])
#
#     if prompt_input := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
#
#         st.session_state.messages.append({"role": "user", "content": prompt_input})
#         with st.chat_message("user"):
#             st.markdown(prompt_input)
#
#         with st.chat_message("assistant"):
#             with st.spinner(f"ƒêang h·ªèi {llm_choice}..."):
#                 try:
#                     config = {"configurable": {"session_id": SESSION_ID}}
#
#                     if st.session_state.vector_store:
#                         # RAG chain tr·∫£ v·ªÅ dictionary (g·ªìm answer v√† context)
#                         # Truy·ªÅn input d∆∞·ªõi d·∫°ng {"input": prompt_input}
#                         result = history_chain.invoke({"input": prompt_input}, config=config)
#                         response = result["answer"]
#                     else:
#                         # Simple chat chain tr·∫£ v·ªÅ AIMessage
#                         # Truy·ªÅn input d∆∞·ªõi d·∫°ng {"input": prompt_input}
#                         response = history_chain.invoke({"input": prompt_input}, config=config).content
#
#                     st.markdown(response)
#                     st.session_state.messages.append({"role": "assistant", "content": response})
#
#                 except Exception as e:
#                     error_msg = f"ƒê√£ x·∫£y ra l·ªói: {e}"
#                     st.error(error_msg)
#                     st.session_state.messages.append({"role": "assistant", "content": error_msg})
#
# else:
#     st.warning("Vui l√≤ng thi·∫øt l·∫≠p kh√≥a API v√† ch·ªçn model h·ª£p l·ªá ƒë·ªÉ s·ª≠ d·ª•ng.")
if llm:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt_input := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):

        st.session_state.messages.append({"role": "user", "content": prompt_input})
        with st.chat_message("user"):
            st.markdown(prompt_input)

        with st.chat_message("assistant"):
            with st.spinner(f"ƒêang h·ªèi {llm_choice}..."):
                try:
                    config = {"configurable": {"session_id": SESSION_ID}}

                    if st.session_state.vector_store:
                        # RAG chain tr·∫£ v·ªÅ dictionary (g·ªìm answer v√† context)
                        result = history_chain.invoke({"input": prompt_input}, config=config)
                        response = result["answer"]
                        context = result.get("context", [])  # L·∫•y Context Documents

                        # --- HI·ªÇN TH·ªä RAG N√ÇNG CAO ---

                        # 1. Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi ch√≠nh
                        st.info("üí° **PH·∫¢N H·ªíI RAG D·ª∞A TR√äN T√ÄI LI·ªÜU:**")
                        st.markdown(response)

                        # 2. Hi·ªÉn th·ªã ngu·ªìn tr√≠ch d·∫´n trong Expander
                        if context:
                            with st.expander("üìö Ngu·ªìn T√†i li·ªáu (Contexts) - B·∫•m ƒë·ªÉ xem"):
                                for i, doc in enumerate(context):
                                    source_page = doc.metadata.get('page', 'N/A')
                                    source_name = os.path.basename(doc.metadata.get('source', 'Unknown File'))

                                    st.markdown(
                                        f"**Chunk {i + 1}** t·ª´ **File:** `{source_name}` (Trang: {source_page})")
                                    # Hi·ªÉn th·ªã n·ªôi dung ƒëo·∫°n tr√≠ch
                                    st.code(doc.page_content[:300] + "...")
                                    st.markdown("---")
                        else:
                            st.warning("Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu ƒë∆∞·ª£c t·∫£i l√™n.")

                    else:
                        # Simple chat chain tr·∫£ v·ªÅ AIMessage
                        response = history_chain.invoke({"input": prompt_input}, config=config).content
                        st.markdown(response)  # Chat th∆∞·ªùng d√πng markdown ƒë∆°n gi·∫£n

                    # L∆∞u ph·∫£n h·ªìi v√†o l·ªãch s·ª≠ chat hi·ªÉn th·ªã (session state)
                    st.session_state.messages.append({"role": "assistant", "content": response})

                except Exception as e:
                    error_msg = f"ƒê√£ x·∫£y ra l·ªói: {e}"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})

else:
    st.warning("Vui l√≤ng thi·∫øt l·∫≠p kh√≥a API v√† ch·ªçn model h·ª£p l·ªá ƒë·ªÉ s·ª≠ d·ª•ng.")